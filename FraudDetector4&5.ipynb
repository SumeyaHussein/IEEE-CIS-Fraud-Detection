{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your path for your fraud train procssed file \n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "#import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "path = r\"/Users/sumeyahussein/Desktop/fraud\"\n",
    "# List all files in the downloaded directory\n",
    "print(\"Files in dataset directory:\", os.listdir(path))\n",
    "\n",
    "csv_path = os.path.join(path, \"fraudprocessed.csv.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('is_fraud', axis=1)\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)\n",
    "\n",
    "#make sure its only oject and int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "#  applying SMOTE to balance the dataset\n",
    "#  generate synthetic samples and balance the dataset for unbalanced classification problems\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Training Decision Tree model with class_weight='balanced' becuase our dataset is unbalanced\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]  # Getting probabilities for the positive class\n",
    "\n",
    "# calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_dt, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_dt, average='weighted')\n",
    "auc = roc_auc_score(y_test, y_pred_proba_dt)  # Calculate AUC\n",
    "\n",
    "\n",
    "# evaluation and printing statment \n",
    "print(\"Decision Tree\\n\")\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "print(\"Decision Tree Precision:\", precision)\n",
    "print(\"Decision Tree Recall:\", recall)\n",
    "print(\"Decision Tree F1 Score:\", f1)\n",
    "print(\"Decision Tree AUC:\", auc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add visulizations \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualization plot\n",
    "plt.figure(figsize=(20, 10))  # Set figure size for clarity\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=X_train.columns if isinstance(X_train, pd.DataFrame) else None,\n",
    "    class_names=[str(c) for c in dt_model.classes_],\n",
    "    filled=True,  \n",
    "    rounded=True,  \n",
    "    fontsize=10, \n",
    "    max_depth=3 # Limiting the depth for better readability \n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "#  generate synthetic samples and balance the dataset for unbalanced classification problems\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Training KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# making predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "y_pred_proba_knn = knn_model.predict_proba(X_test)[:, 1]  # Getting probabilities for the positive class\n",
    "\n",
    "# Calculating all metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "precision = precision_score(y_test, y_pred_knn, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_knn, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_knn, average='weighted')\n",
    "auc = roc_auc_score(y_test, y_pred_proba_knn)  # Calculating  AUC\n",
    "\n",
    "# Evaluation and printing \n",
    "print(\"K-Nearest\\n\")\n",
    "\n",
    "\n",
    "print(\"K-Nearest Accuracy:\", accuracy)\n",
    "print(\"K-Nearest Precision:\", precision)\n",
    "print(\"K-Nearest Recall:\", recall)\n",
    "print(\"K-Nearest F1 Score:\", f1)\n",
    "print(\"K-Nearest AUC:\", auc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_knn))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scaling the data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "# using class_weight='balanced' beucase our dataset is unbalanced \n",
    "lr_model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Calculating the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "precision = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "auc = roc_auc_score(y_test, y_pred_proba_lr)  # Calculating the AUC\n",
    "\n",
    "# Printing  metrics\n",
    "print(\"Logistic Regression\\n\")\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)\n",
    "print(\"Logistic Regression Precision:\", precision)\n",
    "print(\"Logistic Regression Recall:\", recall)\n",
    "print(\"Logistic Regression F1 Score:\", f1)\n",
    "print(\"Logistic Regression AUC:\", auc)\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes​\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Applying SMOTE to balance the dataset\n",
    "#  generate synthetic samples and balance the dataset for unbalanced classification problems\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# making predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Calculating  metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "precision = precision_score(y_test, y_pred_nb, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_nb, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_nb, average='weighted')\n",
    "auc = roc_auc_score(y_test, y_pred_nb)  # Calculate AUC\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"Naive Bayes​\\n\")\n",
    "\n",
    "\n",
    "print(\"Naive Bayes​ Accuracy:\", accuracy)\n",
    "print(\"Naive Bayes​ Precision:\", precision)\n",
    "print(\"Naive Bayes​ Recall:\", recall)\n",
    "print(\"Naive Bayes​ F1 Score:\", f1)\n",
    "print(\"Naive Bayes AUC:\", auc)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# initializing the plot for the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "\n",
    "# models to compare \n",
    "models = [\n",
    "    (\"Logistic Regression\", lr_model, y_pred_proba_lr),\n",
    "    (\"K-Nearest Neighbors\", knn_model, y_pred_proba_knn),\n",
    "    (\"Decision Tree\", dt_model, y_pred_proba_dt),\n",
    "    (\"Naive Bayes\", nb_model, y_pred_nb)\n",
    "]\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for model_name, model, y_pred_proba in models:\n",
    "    #  ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    #  AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Ploting ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "#  random classifier 0.5\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "# Adding  labels and showing \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - All Models')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommened k-nearest neigbor becuase the auc is the highest for this model, lookign only at accuracy may cause issues as our dataset is unbalanced. To combat the unbalanced dataset we used SMOTE- to balance our data and class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add your optimizztion under here\n",
    "# then write a staement for your recommended model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
