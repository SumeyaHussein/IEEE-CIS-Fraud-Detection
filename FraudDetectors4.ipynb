{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is for phase 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nirjala\n",
    "import os\n",
    "import pandas as pd\n",
    "#import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "path = r\"/Users/nirjalagurung/fraud\"\n",
    "# List all files in the downloaded directory\n",
    "print(\"Files in dataset directory:\", os.listdir(path))\n",
    "\n",
    "# Load the CSV file (assuming it is named fraudTest.csv in the downloaded files)\n",
    "csv_path = os.path.join(path, \"fraudTest.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spandana runs this <using fraudTrain>\n",
    "import os\n",
    "import pandas as pd\n",
    "#import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "path = r\"C:\\Users\\Thinking1\\vsc_workspace\\FDS\"\n",
    "\n",
    "# List all files in the downloaded directory\n",
    "print(\"Files in dataset directory:\", os.listdir(path))\n",
    "\n",
    "# Load the CSV file (assuming it is named fraudTest.csv in the downloaded files)\n",
    "csv_path = os.path.join(path, \"fraudTrain.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Path to dataset files: /Users/sumeyahussein/.cache/kagglehub/datasets/kartik2112/fraud-detection/versions/1\n",
      "Files in dataset directory: ['fraudTrain.csv', 'fraudTest.csv']\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2020-06-21 12:14:25  2291163933867244   \n",
      "1           1   2020-06-21 12:14:33  3573030041201292   \n",
      "2           2   2020-06-21 12:14:53  3598215285024754   \n",
      "3           3   2020-06-21 12:15:15  3591919803438423   \n",
      "4           4   2020-06-21 12:15:17  3526826139003047   \n",
      "\n",
      "                               merchant        category    amt   first  \\\n",
      "0                 fraud_Kirlin and Sons   personal_care   2.86    Jeff   \n",
      "1                  fraud_Sporer-Keebler   personal_care  29.84  Joanne   \n",
      "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness  41.28  Ashley   \n",
      "3                     fraud_Haley Group        misc_pos  60.05   Brian   \n",
      "4                 fraud_Johnston-Casper          travel   3.19  Nathan   \n",
      "\n",
      "       last gender                       street  ...      lat      long  \\\n",
      "0   Elliott      M            351 Darlene Green  ...  33.9659  -80.9355   \n",
      "1  Williams      F             3638 Marsh Union  ...  40.3207 -110.4360   \n",
      "2     Lopez      F         9333 Valentine Point  ...  40.6729  -73.5365   \n",
      "3  Williams      M  32941 Krystal Mill Apt. 552  ...  28.5697  -80.8191   \n",
      "4    Massey      M     5783 Evan Roads Apt. 465  ...  44.2529  -85.0170   \n",
      "\n",
      "   city_pop                     job         dob  \\\n",
      "0    333497     Mechanical engineer  1968-03-19   \n",
      "1       302  Sales professional, IT  1990-01-17   \n",
      "2     34496       Librarian, public  1970-10-21   \n",
      "3     54767            Set designer  1987-07-25   \n",
      "4      1126      Furniture designer  1955-07-06   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714   \n",
      "1  324cc204407e99f51b0d6ca0055005e7  1371816873  39.450498 -109.960431   \n",
      "2  c81755dbbbea9d5c77f094348a7579be  1371816893  40.495810  -74.196111   \n",
      "3  2159175b9efe66dc301f149d3d5abf8c  1371816915  28.812398  -80.883061   \n",
      "4  57ff021bd3f328f8738bb535c302a31b  1371816917  44.959148  -85.884734   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List all files in the downloaded directory\n",
    "print(\"Files in dataset directory:\", os.listdir(path))\n",
    "\n",
    "# Load the CSV file (assuming it is named fraudTest.csv in the downloaded files)\n",
    "csv_path = os.path.join(path, \"fraudTest.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to be processed:\n",
      "['trans_date_trans_time', 'merchant', 'category', 'street', 'city', 'state', 'job']\n",
      "\n",
      "High cardinality columns (more than 50 unique values):\n",
      "['trans_date_trans_time', 'merchant', 'street', 'city', 'job']\n",
      "\n",
      "Low cardinality columns (50 or fewer unique values):\n",
      "['category', 'state']\n",
      "Applying label encoding to high cardinality column: trans_date_trans_time\n",
      "Applying label encoding to high cardinality column: merchant\n",
      "Applying label encoding to high cardinality column: street\n",
      "Applying label encoding to high cardinality column: city\n",
      "Applying label encoding to high cardinality column: job\n",
      "\n",
      "Applying one-hot encoding to low cardinality columns...\n",
      "\n",
      "Final DataFrame shape: (555719, 79)\n",
      "   Unnamed: 0  trans_date_trans_time            cc_num  merchant    amt  \\\n",
      "0           0                      0  2291163933867244       319   2.86   \n",
      "1           1                      1  3573030041201292       591  29.84   \n",
      "2           2                      2  3598215285024754       611  41.28   \n",
      "3           3                      3  3591919803438423       222  60.05   \n",
      "4           4                      4  3526826139003047       292   3.19   \n",
      "\n",
      "  gender  street  city    zip      lat  ...  state_SD  state_TN  state_TX  \\\n",
      "0      M     341   157  29209  33.9659  ...         0         0         0   \n",
      "1      F     354    16  84002  40.3207  ...         0         0         0   \n",
      "2      F     865    61  11710  40.6729  ...         0         0         0   \n",
      "3      M     320   764  32780  28.5697  ...         0         0         0   \n",
      "4      M     548   247  49632  44.2529  ...         0         0         0   \n",
      "\n",
      "   state_UT  state_VA  state_VT  state_WA  state_WI  state_WV  state_WY  \n",
      "0         0         0         0         0         0         0         0  \n",
      "1         1         0         0         0         0         0         0  \n",
      "2         0         0         0         0         0         0         0  \n",
      "3         0         0         0         0         0         0         0  \n",
      "4         0         0         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "#encoding\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv(\"fraudTrain.csv\")\n",
    "\n",
    "# Drop only the specified columns that should not be processed\n",
    "drop_columns = ['first', 'last', 'dob', 'trans_num']\n",
    "df = df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "# List the categorical columns that need to be processed (manually given in the problem)\n",
    "categorical_columns = ['trans_date_trans_time', 'merchant', 'category', 'street', 'city', 'state', 'job']\n",
    "\n",
    "# Print the categorical columns to confirm\n",
    "print(\"Categorical columns to be processed:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Check cardinality\n",
    "# Calculate the number of unique values for each categorical column\n",
    "cardinality = df[categorical_columns].nunique()\n",
    "\n",
    "# Separate columns based on their cardinality\n",
    "high_cardinality = cardinality[cardinality > 50].index\n",
    "low_cardinality = cardinality[cardinality <= 50].index\n",
    "\n",
    "# Print identified columns\n",
    "print(\"\\nHigh cardinality columns (more than 50 unique values):\")\n",
    "print(high_cardinality.tolist())\n",
    "print(\"\\nLow cardinality columns (50 or fewer unique values):\")\n",
    "print(low_cardinality.tolist())\n",
    "\n",
    "# Apply label encoding for high-cardinality columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in high_cardinality:\n",
    "    print(f\"Applying label encoding to high cardinality column: {col}\")\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Apply one-hot encoding for low-cardinality columns\n",
    "print(\"\\nApplying one-hot encoding to low cardinality columns...\")\n",
    "df_encoded = pd.get_dummies(df, columns=low_cardinality, drop_first=True)\n",
    "\n",
    "# Inspect the updated DataFrame\n",
    "print(\"\\nFinal DataFrame shape:\", df_encoded.shape)\n",
    "print(df_encoded.head())\n",
    "\n",
    "#uncomment to write processed data to new file\n",
    "#df.to_csv(\"fraudTrain_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized numerical columns:\n",
      "   Unnamed: 0  trans_date_trans_time    cc_num  merchant       amt    street  \\\n",
      "0    0.000000               0.000000  0.000459  0.460983  0.000082  0.369447   \n",
      "1    0.000002               0.000002  0.000716  0.854046  0.001267  0.383532   \n",
      "2    0.000004               0.000004  0.000721  0.882948  0.001769  0.937161   \n",
      "3    0.000005               0.000006  0.000719  0.320809  0.002594  0.346696   \n",
      "4    0.000007               0.000007  0.000706  0.421965  0.000096  0.593716   \n",
      "\n",
      "       city       zip       lat      long  city_pop       job     unix_time  \\\n",
      "0  0.185142  0.283305  0.305255  0.867121  0.114727  0.576520  0.000000e+00   \n",
      "1  0.018868  0.838654  0.444423  0.565239  0.000096  0.821803  4.785402e-07   \n",
      "2  0.071934  0.105945  0.452136  0.942836  0.011860  0.542977  1.674891e-06   \n",
      "3  0.900943  0.319498  0.187080  0.868312  0.018834  0.853249  2.990876e-06   \n",
      "4  0.291274  0.490300  0.530537  0.825355  0.000379  0.410901  3.110511e-06   \n",
      "\n",
      "   merch_lat  merch_long  \n",
      "0   0.313922    0.857112  \n",
      "1   0.428589    0.568706  \n",
      "2   0.450526    0.927355  \n",
      "3   0.205343    0.860298  \n",
      "4   0.544191    0.810140  \n"
     ]
    }
   ],
   "source": [
    "#normalizing scaling numerical columns \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv(\"fraudTrain.csv\")\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "columns_to_drop = ['dob','trans_num']\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Identifying numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_columns = numerical_columns.drop('is_fraud', errors='ignore')\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizing the numerical columns\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Inspect the updated DataFrame\n",
    "print(\"Normalized numerical columns:\")\n",
    "print(df[numerical_columns].head())\n",
    "\n",
    "# uncomment to write to the new file from codeblock above too\n",
    "#df.to_csv(\"fraudTrain_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (444575, 18)\n",
      "Test set size: (111144, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('is_fraud', axis=1)\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               float64\n",
      "trans_date_trans_time    float64\n",
      "cc_num                   float64\n",
      "merchant                 float64\n",
      "category                  object\n",
      "amt                      float64\n",
      "gender                    object\n",
      "street                   float64\n",
      "city                     float64\n",
      "state                     object\n",
      "zip                      float64\n",
      "lat                      float64\n",
      "long                     float64\n",
      "city_pop                 float64\n",
      "job                      float64\n",
      "unix_time                float64\n",
      "merch_lat                float64\n",
      "merch_long               float64\n",
      "dtype: object\n",
      "Unnamed: 0               float64\n",
      "trans_date_trans_time    float64\n",
      "cc_num                   float64\n",
      "merchant                 float64\n",
      "category                  object\n",
      "amt                      float64\n",
      "gender                    object\n",
      "street                   float64\n",
      "city                     float64\n",
      "state                     object\n",
      "zip                      float64\n",
      "lat                      float64\n",
      "long                     float64\n",
      "city_pop                 float64\n",
      "job                      float64\n",
      "unix_time                float64\n",
      "merch_lat                float64\n",
      "merch_long               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the new-cleaned csv files for model selection and evalution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 4: Model Selection and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your models using a train/test split and apply appropriate\n",
    "evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement evaluation metrics not included in standard libraries if\n",
    "applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a baseline model and test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test at least three categories of learning models, such as:\n",
    "✓ Information-based models: Decision Trees, Random Forest\n",
    "✓ Similarity-based models: K-Nearest Neighbors\n",
    "✓ Probability-based models: Naive Bayes\n",
    "✓ Error-based models: Linear Regression, Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9966709853883251\n",
      "Decision Tree Precision: 0.9967414907415487\n",
      "Decision Tree Recall: 0.9966709853883251\n",
      "Decision Tree F1 Score: 0.9967052881557187\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    110718\n",
      "           1       0.56      0.59      0.57       426\n",
      "\n",
      "    accuracy                           1.00    111144\n",
      "   macro avg       0.78      0.79      0.79    111144\n",
      "weighted avg       1.00      1.00      1.00    111144\n",
      "\n",
      "Confusion Matrix:\n",
      " [[110524    194]\n",
      " [   176    250]]\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Training Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# making  predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_dt, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_dt, average='weighted')\n",
    "\n",
    "# printing \n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "print(\"Decision Tree Precision:\", precision)\n",
    "print(\"Decision Tree Recall:\", recall)\n",
    "print(\"Decision Tree F1 Score:\", f1)\n",
    "\n",
    "# confusion matrix\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add visulizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Accuracy: 0.9968959188080329\n",
      "K-Nearest Neighbors Precision: 0.9962921165016478\n",
      "K-Nearest Neighbors Recall: 0.9968959188080329\n",
      "K-Nearest Neighbors F1 Score: 0.9962924735472499\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    110718\n",
      "           1       0.72      0.31      0.44       426\n",
      "\n",
      "    accuracy                           1.00    111144\n",
      "   macro avg       0.86      0.66      0.72    111144\n",
      "weighted avg       1.00      1.00      1.00    111144\n",
      "\n",
      "Confusion Matrix:\n",
      " [[110665     53]\n",
      " [   292    134]]\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scaling  the data for KNN\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  \n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "precision = precision_score(y_test, y_pred_knn, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred_knn, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_knn, average='weighted')\n",
    "\n",
    "\n",
    "print(\"K-Nearest Neighbors Accuracy:\", accuracy)\n",
    "print(\"K-Nearest Neighbors Precision:\", precision)\n",
    "print(\"K-Nearest Neighbors Recall:\", recall)\n",
    "print(\"K-Nearest Neighbors F1 Score:\", f1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_knn))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add vizulization \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy 1.0\n",
      "test_accuracy 1.0\n",
      "cross_validation 0.9999910026429737\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(X_train)\n",
    "x_test=scaler.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr=LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_train=lr.predict(x_train)\n",
    "y_test=lr.predict(x_test)\n",
    "print(\"test_accuracy\",accuracy_score(y_train,y_train))\n",
    "print(\"test_accuracy\",accuracy_score(y_test,y_test))\n",
    "print(\"cross_validation\",cross_val_score(lr,x_train,y_train,cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add vizulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing model viz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply hyper-parameter optimization (preferably using grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and justify the recommended model based on performance,\n",
    "interpretability, efficiency, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 5: Communicate Findings and Recommend an Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze model results, demonstrate relationships between features\n",
    "and the target variable, and recommend actions based on your find-\n",
    "ings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
